{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LV9_Programmieren.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/TerminologieHilfsmitel/blob/master/LV9_Programmieren.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqQemUMhFOnB",
        "colab_type": "text"
      },
      "source": [
        "**Um dieses Notebook in Google Colab zu öffnen, klicken Sie bitte direkt unter das Logo in der Mitte über dieser Zeile. Damit sollte sich dann das Notebook in einem neuen Tab in Google Colab öffnen.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvZfzSn7bXVl",
        "colab_type": "text"
      },
      "source": [
        "# Lektion 0: Dieses Notebook speichern \n",
        "\n",
        "Gehen Sie auf \"File\" (\"Datei\") und speichern Sie eine lokale Kopie dieses Notebooks, entweder auf Google Drive oder in Ihrem Github-Konto. Alternativ können Sie das Notebook auch herunterladen und lokal bearbeiten - Achtung dazu muss Python lokal installiert werden und die nachstehenden Code-Abschnitte direkt in Python ausgeführt werden. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7PUvU1f4H7C",
        "colab_type": "text"
      },
      "source": [
        "In diesem Notebook finden Sie ausführbaren Programmiercode, den Sie direkt in Ihrem Browser ausführen können. Code ist immer durch einen grauen Rahmen markiert. Wenn Sie in dem grauen Bereich klicken, wird eine \"Play\" Schalfläche sichtbar.\n",
        "\n",
        "**Praktische Übungen sind nachstehend als fett gedruckt markiert.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHzL2Qjivc0N",
        "colab_type": "text"
      },
      "source": [
        "# Lektion 1: TF-IDF selbst programmieren\n",
        "\n",
        "Diese Lektion zeigt Ihnen, wie Sie TF-IDF selbst in Python implementieren können. Um alle notwendigen Libraries zu importieren, lassen Sie bitte den nachstehenden Code als erstes laufen.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKH8xwY1GZrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install stopwords\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "import math\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stoplist = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EP7tdfxorzF",
        "colab_type": "text"
      },
      "source": [
        "Zuerst erstellen wir einen Beispielkorpus mit unseren drei Sätzen aus der Vorlesung.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mitaqd1HpTLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence1 = \"Shipment of gold damaged in a fire.\" \n",
        "sentence2= \"Delivery of silver arrived in a silver truck.\" \n",
        "sentence3 = \"Shipment of gold arrived in a truck.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwxn4kh2qWlA",
        "colab_type": "text"
      },
      "source": [
        "Im Sinne der linguistischen Vorverarbeitung, werden wir als erste alle Worte klein schreiben. Das können wir in Python automatisch mithilfe der Funktion `lower()` erreichen. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa8bjpExICCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence1 = sentence1.lower()\n",
        "sentence2 = sentence2.lower()\n",
        "sentence3 = sentence3.lower()\n",
        "print(sentence1, \"\\n\", sentence2, \"\\n\", sentence3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obHTGjl8H94w",
        "colab_type": "text"
      },
      "source": [
        "Normalerweise werden weitere Vorverarbeitungsschrittet durchgeführt. Heute bearbeiten wir unsere Sätze wie folgt: \n",
        "* Tokenisierung\n",
        "* Entfernen von Stoppworten \n",
        "* Lemmatisierung\n",
        "* Entfernen von Interpunktion\n",
        "\n",
        "Dafür definieren wir eine eigene Funktion namens `preprocessing()`. Eine Liste zu ändern die man gerade mit einer `for`-Schleife durchläuft ist meine keine gute Idee. Daher ist es einfacher zu überprüfen ob eine Bedingung nicht erfüllt ist mithilfe von `not` wie folgt dargestellt. Also wenn der Token nicht in der Stoppwortliste ist und auch keine Interpunktion ist, wird er lemmatisiert und der Liste `lemmas` hinzugefügt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhqzgf6zIW_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(text): \n",
        "  lemmas = []\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  for token in tokens: \n",
        "    if token not in stoplist and token not in string.punctuation: \n",
        "      lemmas.append(lemmatizer.lemmatize(token))\n",
        "  return lemmas \n",
        "\n",
        "lemmas1 = preprocessing(sentence1)\n",
        "lemmas2 = preprocessing(sentence2)\n",
        "lemmas3 = preprocessing(sentence3)\n",
        "print(lemmas1, \"\\n\", lemmas2, \"\\n\", lemmas3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdicI9HKIjnb",
        "colab_type": "text"
      },
      "source": [
        "Jetzt müssen wir eine Liste aller Worte in unserem Korpus erstellen. Wir brauchen hier zwei Listen: 1) eine Liste mit allen Worten inklusive Doppelnennungen um die Häufigkeit zu zählen, 2) eine Liste des Vokabulars ohne Mehrfachnenungen. Um jedes Element in einenr Liste nur einmal vorkommen zu lassen kann man eine Liste einfach in ein `set` konvertieren. Dazu fügen wir alle Lemma-Listen zusammen und bearbeiten die Gesamtheit der Worte in unserem Korpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3E-MzOwIl6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = []\n",
        "vocab = lemmas1 + lemmas2\n",
        "vocab = vocab + lemmas3\n",
        "print(\"Wortliste mit Doppelnennungen:\", vocab)\n",
        "print(\"Wortliste ohne Doppelnennungen:\", set(vocab))\n",
        "unique_vocab = set(vocab)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jai6v73YTrgM",
        "colab_type": "text"
      },
      "source": [
        "Jetzt können wir die Termhäufigkeit berechnen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqGlzgDLTPQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_tf(vocab_list):\n",
        "  \"\"\"Berechnet die Termhäufigkeit im gesamten Korpus\n",
        "  auf Basis der gesamten Wortliste inklusive Doppelnennungen\n",
        "  und es wird ein TF-Dictionary zurückgegeben\"\"\"\n",
        "  tf_dictionary = dict()\n",
        "  for word in vocab_list: \n",
        "    if word in tf_dictionary: \n",
        "      #Eine Kurzschreibweise von tf_dictionary[word] = tf_dictionary[word] + 1\n",
        "      tf_dictionary[word] += 1\n",
        "    else: \n",
        "      tf_dictionary[word] = 1\n",
        "  return tf_dictionary\n",
        "\n",
        "tf = compute_tf(vocab)\n",
        "print(tf)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqEKRgVGUHep",
        "colab_type": "text"
      },
      "source": [
        "Für IDF müssen wir zuerst die Dokumenthäufigkeit berechnen.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQO8f2hDULqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_df(unique_vocab, documents):\n",
        "  \"\"\"Berechnet die Dokumenthäufigkeit auf der Basis der in einer Liste gespeicherten Dokumente \n",
        "  und einer Liste von Worten ohne Doppelnennungen des gesamten Korpus\n",
        "  und gibt die Werte als dict() zurück\"\"\"\n",
        "  df_dictionary = dict()\n",
        "  for word in unique_vocab:\n",
        "    for document in documents: \n",
        "      if word in document:\n",
        "        if word in df_dictionary: \n",
        "          df_dictionary[word] += 1\n",
        "        else: \n",
        "          df_dictionary[word] = 1 \n",
        "  return df_dictionary\n",
        "\n",
        "#Wir fügen alle Lemmalisten zu einem Korpus zusammen. Dieser unterscheidet sich von der Vokabularliste insofern \n",
        "# dass die einzelnen Vorkomnisse der Worte dem Dokument zuordenbar sind.\n",
        "corpus = []\n",
        "corpus.append(lemmas1)\n",
        "corpus.append(lemmas2)\n",
        "corpus.append(lemmas3)\n",
        "df = compute_df(unique_vocab, corpus)\n",
        "\n",
        "print(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoNOhtqxZkfC",
        "colab_type": "text"
      },
      "source": [
        "Jetzt haben wir alle Informationen um die inverse Dokumenthäufigkeit zu berechnen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdjhRzgdZpMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_idf(df_dictionary):\n",
        "    \"\"\"Berechnung der inversen Dokumenthäufigkeit auf Basis der \n",
        "    Dokumenthäufigkeit und Anzahl der Dokumente im Korpus\n",
        "    und gibt die Werte als dict() zurück\"\"\"\n",
        "    idf_dict = {}\n",
        "    for word in df_dictionary:\n",
        "      idf_dict[word] = math.log10(len(corpus) / df_dictionary[word])\n",
        "    return idf_dict\n",
        "  \n",
        "idf = compute_idf(df)\n",
        "print(idf)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_4bMj9dbOOB",
        "colab_type": "text"
      },
      "source": [
        "Und jetzt haben wir alles um TF-IDF zu berechnen. Wie wir wissen, wird TF-IDF auf Basis der einzelnen Dokumente berechnet und daher erhalten wir drei Listen, eine für jedes Dokument"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5gPzlymbRbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_tfidf(lemmas, idf):\n",
        "  \"\"\"Berechnung von TFIDF auf Basis der Termhäufigkeit und inversen Dokumenthäufigkeit\n",
        "  und gibt die berechneten Werte als dict() zurück\"\"\"\n",
        "  tfidf_dict = dict()\n",
        "  #For each word in the review, we multiply its tf and its idf.\n",
        "  for word in lemmas:\n",
        "    tfidf_dict[word] = lemmas.count(word) * idf[word]\n",
        "  return tfidf_dict\n",
        "\n",
        "tfidf_doc1 = compute_tfidf(lemmas1, idf)\n",
        "tfidf_doc2 = compute_tfidf(lemmas2, idf)\n",
        "tfidf_doc3 = compute_tfidf(lemmas3, idf)\n",
        "print(tfidf_doc1)\n",
        "print(tfidf_doc2)\n",
        "print(tfidf_doc3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il4fbXIzia_s",
        "colab_type": "text"
      },
      "source": [
        "**Übung: Berechnen Sie den TF-IDF Wert für einen anderen Korpus - also andere Sätze**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlOyIFSoimbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Ihr Code hier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoEwbJ0PpZ0d",
        "colab_type": "text"
      },
      "source": [
        "# Lektion 2: TF-IDF mit bereitgestelleter Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7NDqyefpe3W",
        "colab_type": "text"
      },
      "source": [
        "Anstatt TF-IDF selbst zu programmieren, können wir auch hier eine bereits bestehende Library für maschinelles Lernen namens ´sklearn´ verwenden. Wie Sie sehen wird hier automatisch tokeenisiert und lemmatisiert. Die Stoppwortliste üebrgeben wir bei der Initialisierung des vectorizer um Stoppworte zu entfernen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO0GA4uVp2wH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Eine Instanz der Klasse TfidfVectorizer muss erstellt werden für unsere spezifische Anwendung\n",
        "vectorizer = TfidfVectorizer(stop_words=stoplist)\n",
        "corpus_new = [\n",
        "              'Shipment of gold damaged in a fire.',\n",
        "              'Delivery of silver arrived in a silver truck.',\n",
        "              'Shipment of gold arrived in a truck.']\n",
        "\n",
        "#Diese Zeile trainiert unsere Instanz und erstellt eine TF-IDF Matrix die ausgegeben wird\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus_new)\n",
        "wort_index = vectorizer.vocabulary_\n",
        "idf = vectorizer.idf_\n",
        "print(wort_index)\n",
        "print(\"Matrix: Dokument, Wortindex, TFID \\n\", tfidf_matrix)\n",
        "print(\"IDF wird hier mit einem anderen Logarithmus berechnet: \", dict(zip(vectorizer.get_feature_names(), idf)))\n",
        "print(\"TF-IDF daher auch etwas anders aber dieselbe Verteilung: \", dict(zip(vectorizer.get_feature_names(), np.asarray(tfidf_matrix.sum(axis=0)).ravel())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UZcFQZ1h7p4",
        "colab_type": "text"
      },
      "source": [
        "# Lektion 3: Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KjnxyB-u7tR",
        "colab_type": "text"
      },
      "source": [
        "Dieselbe Bibliothek, sklearn, kann auch für Clustering verwendet werden. Dazu verwenden wir die erstellten TF-IDF Vektoren. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-R5JojFvCnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def print_clusters(number_clusters, kmeans):\n",
        "  order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
        "  terms = vectorizer.get_feature_names()\n",
        "  for i in range(number_clusters):\n",
        "    print(\"Cluster %d:\" % i, end='')\n",
        "    for ind in order_centroids[i,:3]:\n",
        "      print(' %s' % terms[ind], end='')\n",
        "    print()\n",
        "\n",
        "\n",
        "# Wir müssen die Anzahl an erwarteten Clusterin eingeben - die optimale Zahl ist experimentell zu testen \n",
        "number_clusters = 2\n",
        "kmeans = KMeans(n_clusters=number_clusters, random_state=0).fit(tfidf_matrix)\n",
        "print_clusters(number_clusters, kmeans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IaT3KYCrKjr",
        "colab_type": "text"
      },
      "source": [
        "**Übung: Passen Sie den Algorithmus oben auf Ihre eigenen Sätze an.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb_glsplrR23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Ihr Code hier"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}