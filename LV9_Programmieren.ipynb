{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LV9_Programmieren.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/TerminologieHilfsmitel/blob/master/LV9_Programmieren.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqQemUMhFOnB",
        "colab_type": "text"
      },
      "source": [
        "**Um dieses Notebook in Google Colab zu öffnen, klicken Sie bitte direkt unter das Logo in der Mitte über dieser Zeile. Damit sollte sich dann das Notebook in einem neuen Tab in Google Colab öffnen.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvZfzSn7bXVl",
        "colab_type": "text"
      },
      "source": [
        "# Lektion 0: Dieses Notebook speichern \n",
        "\n",
        "Gehen Sie auf \"File\" (\"Datei\") und speichern Sie eine lokale Kopie dieses Notebooks, entweder auf Google Drive oder in Ihrem Github-Konto. Alternativ können Sie das Notebook auch herunterladen und lokal bearbeiten - Achtung dazu muss Python lokal installiert werden und die nachstehenden Code-Abschnitte direkt in Python ausgeführt werden. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7PUvU1f4H7C",
        "colab_type": "text"
      },
      "source": [
        "In diesem Notebook finden Sie ausführbaren Programmiercode, den Sie direkt in Ihrem Browser ausführen können. Code ist immer durch einen grauen Rahmen markiert. Wenn Sie in dem grauen Bereich klicken, wird eine \"Play\" Schalfläche sichtbar.\n",
        "\n",
        "**Praktische Übungen sind nachstehend als fett gedruckt markiert.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHzL2Qjivc0N",
        "colab_type": "text"
      },
      "source": [
        "# Lektion 1: TF-IDF selbst programmieren\n",
        "\n",
        "Diese Lektion zeigt Ihnen, wie Sie TF-IDF selbst in Python implementieren können. Um alle notwendigen Libraries zu importieren, lassen Sie bitte den nachstehenden Code als erstes laufen.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKH8xwY1GZrv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "37b9e50e-bc9a-4553-8985-bd90c918237b"
      },
      "source": [
        "!pip install stopwords\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "import math\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stoplist = stopwords.words('english')"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stopwords in /usr/local/lib/python3.6/dist-packages (0.1.3)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EP7tdfxorzF",
        "colab_type": "text"
      },
      "source": [
        "Zuerst erstellen wir einen Beispielkorpus mit unseren drei Sätzen aus der Vorlesung. Wir erstellen die drei Sätze gleich als Liste, damit wir eine Variable haben, die wir bearbeiten (linguistische Vorverarbeitung). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mitaqd1HpTLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence1 = \"Shipment of gold damaged in a fire.\" \n",
        "sentence2= \"Delivery of silver arrived in a silver truck.\" \n",
        "sentence3 = \"Shipment of gold arrived in a truck.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwxn4kh2qWlA",
        "colab_type": "text"
      },
      "source": [
        "Im Sinne der linguistischen Vorverarbeitung, werden wir als erste alle Worte klein schreiben. Das können wir in Python automatisch mithilfe der Funktion `lower()` erreichen. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa8bjpExICCG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9e5249c7-0089-4a97-cdde-65c4c0cb5eaa"
      },
      "source": [
        "sentence1 = sentence1.lower()\n",
        "sentence2 = sentence2.lower()\n",
        "sentence3 = sentence3.lower()\n",
        "print(sentence1, \"\\n\", sentence2, \"\\n\", sentence3)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shipment of gold damaged in a fire. \n",
            " delivery of silver arrived in a silver truck. \n",
            " shipment of gold arrived in a truck.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obHTGjl8H94w",
        "colab_type": "text"
      },
      "source": [
        "Normalerweise werden weitere Vorverarbeitungsschrittet durchgeführt. Heute bearbeiten wir unsere Sätze wie folgt: \n",
        "* Tokenisierung\n",
        "* Entfernen von Stoppworten \n",
        "* Lemmatisierung\n",
        "* Entfernen von Interpunktion\n",
        "\n",
        "Dafür definieren wir eine eigene Funktion namens `preprocessing()`. Eine Liste zu ändern die man gerade mit einer `for`-Schleife durchläuft ist meine keine gute Idee. Daher ist es einfacher zu überprüfen ob eine Bedingung nicht erfüllt ist mithilfe von `not` wie folgt dargestellt. Also wenn der Token nicht in der Stoppwortliste ist und auch keine Interpunktion ist, wird er lemmatisiert und der Liste `lemmas` hinzugefügt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhqzgf6zIW_b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4b085f87-c28e-4fa6-8ca6-b5ed44ee6061"
      },
      "source": [
        "def preprocessing(text): \n",
        "  lemmas = []\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  for token in tokens: \n",
        "    if token not in stoplist and token not in string.punctuation: \n",
        "      lemmas.append(lemmatizer.lemmatize(token))\n",
        "  return lemmas \n",
        "\n",
        "lemmas1 = preprocessing(sentence1)\n",
        "lemmas2 = preprocessing(sentence2)\n",
        "lemmas3 = preprocessing(sentence3)\n",
        "print(lemmas1, \"\\n\", lemmas2, \"\\n\", lemmas3)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['shipment', 'gold', 'damaged', 'fire'] \n",
            " ['delivery', 'silver', 'arrived', 'silver', 'truck'] \n",
            " ['shipment', 'gold', 'arrived', 'truck']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdicI9HKIjnb",
        "colab_type": "text"
      },
      "source": [
        "Jetzt müssen wir eine Liste alle Worte in unserem Korpus erstellen. Um Mehrfachnennungen eines Wortes zu elminieren, kann man eine Liste in ein `set` konvertieren. Dazu fügen wir alle Lemma-Listen in eine Liste und konvertieren das ganze in ein Set. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3E-MzOwIl6Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5290616e-e6d6-4d1e-d84b-2a7837a85ab1"
      },
      "source": [
        "vocab = []\n",
        "vocab = lemmas1 + lemmas2\n",
        "vocab = vocab + lemmas3\n",
        "print(\"Wortliste mit Doppelnennungen:\", vocab)\n",
        "print(\"Wortliste ohne Doppelnennungen:\", set(vocab))\n",
        "unique_vocab = set(vocab)\n"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wortliste mit Doppelnennungen: ['shipment', 'gold', 'damaged', 'fire', 'delivery', 'silver', 'arrived', 'silver', 'truck', 'shipment', 'gold', 'arrived', 'truck']\n",
            "Wortliste ohne Doppelnennungen: {'silver', 'truck', 'fire', 'arrived', 'shipment', 'damaged', 'delivery', 'gold'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqGlzgDLTPQi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11a5e46f-aa70-450b-b327-08df771b88ce"
      },
      "source": [
        "def compute_tf(vocab_list):\n",
        "  \"\"\"Berechnet die Termhäufigkeit im gesamten Korpus\n",
        "  auf Basis der gesamten Wortliste inklusive Doppelnennungen\n",
        "  und gibt die Werte als dict() zurück\"\"\"\n",
        "  tf_dictionary = dict()\n",
        "  for word in vocab_list: \n",
        "    if word in tf_dictionary: \n",
        "      #Eine Kurzschreibweise von tf_dictionary[word] = tf_dictionary[word] + 1\n",
        "      tf_dictionary[word] += 1\n",
        "    else: \n",
        "      tf_dictionary[word] = 1\n",
        "  return tf_dictionary\n",
        "tf = compute_tf(vocab)\n",
        "print(tf)\n"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'shipment': 2, 'gold': 2, 'damaged': 1, 'fire': 1, 'delivery': 1, 'silver': 2, 'arrived': 2, 'truck': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqEKRgVGUHep",
        "colab_type": "text"
      },
      "source": [
        "Für IDF müssen wir zuerst die Dokumenthäufigkeit berechnen.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQO8f2hDULqL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48e32200-37a1-49d0-a6b1-d22f67471471"
      },
      "source": [
        "def compute_df(unique_vocab, documents):\n",
        "  \"\"\"Berechnet die Dokumenthäufigkeit auf der Basis der in einer Liste gespeicherten Dokumente \n",
        "  und einer Liste von Worten ohne Doppelnennungen des gesamten Korpus\n",
        "  und gibt die Werte als dict() zurück\"\"\"\n",
        "  df_dictionary = dict()\n",
        "  for word in unique_vocab:\n",
        "    for document in documents: \n",
        "      if word in document:\n",
        "        if word in df_dictionary: \n",
        "          df_dictionary[word] += 1\n",
        "        else: \n",
        "          df_dictionary[word] = 1 \n",
        "  return df_dictionary\n",
        "\n",
        "corpus = []\n",
        "corpus.append(lemmas1)\n",
        "corpus.append(lemmas2)\n",
        "corpus.append(lemmas3)\n",
        "df = compute_df(unique_vocab, corpus)\n",
        "\n",
        "print(df)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'silver': 1, 'truck': 2, 'fire': 1, 'arrived': 2, 'shipment': 2, 'damaged': 1, 'delivery': 1, 'gold': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoNOhtqxZkfC",
        "colab_type": "text"
      },
      "source": [
        "Jetzt haben wir alle Informationen um die inverse Dokumenthäufigkeit zu berechnen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdjhRzgdZpMn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bd6997fa-3f3a-4e04-906b-78e34f692582"
      },
      "source": [
        "def compute_idf(df_dictionary):\n",
        "    \"\"\"Berechnung der inversen Dokumenthäufigkeit auf Basis der \n",
        "    Dokumenthäufigkeit und Anzahl der Dokumente im Korpus\n",
        "    und gibt die Werte als dict() zurück\"\"\"\n",
        "    idf_dict = {}\n",
        "    for word in df_dictionary:\n",
        "      idf_dict[word] = math.log10(len(corpus) / df_dictionary[word])\n",
        "    return idf_dict\n",
        "  \n",
        "idf = compute_idf(df)\n",
        "print(idf)\n"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'silver': 0.47712125471966244, 'truck': 0.17609125905568124, 'fire': 0.47712125471966244, 'arrived': 0.17609125905568124, 'shipment': 0.17609125905568124, 'damaged': 0.47712125471966244, 'delivery': 0.47712125471966244, 'gold': 0.17609125905568124}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_4bMj9dbOOB",
        "colab_type": "text"
      },
      "source": [
        "Und jetzt haben wir alles um TF-IDF zu berechnen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5gPzlymbRbL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "069634db-9619-4345-b77a-9930db551645"
      },
      "source": [
        "def compute_tfidf(lemmas1, idf):\n",
        "  \"\"\"Berechnung von TFIDF auf Basis der Termhäufigkeit und inversen Dokumenthäufigkeit\n",
        "  und gibt die berechneten Werte als dict() zurück\"\"\"\n",
        "  tfidf_dict = dict()\n",
        "  #For each word in the review, we multiply its tf and its idf.\n",
        "  for word in lemmas1:\n",
        "    tfidf_dict[word] = lemmas1.count(word) * idf[word]\n",
        "  return tfidf_dict\n",
        "\n",
        "tfidf_doc1 = compute_tfidf(lemmas1, idf)\n",
        "tfidf_doc2 = compute_tfidf(lemmas2, idf)\n",
        "tfidf_doc3 = compute_tfidf(lemmas3, idf)\n",
        "print(tfidf_doc1)\n",
        "print(tfidf_doc2)\n",
        "print(tfidf_doc3)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'shipment': 0.17609125905568124, 'gold': 0.17609125905568124, 'damaged': 0.47712125471966244, 'fire': 0.47712125471966244}\n",
            "{'delivery': 0.47712125471966244, 'silver': 0.9542425094393249, 'arrived': 0.17609125905568124, 'truck': 0.17609125905568124}\n",
            "{'shipment': 0.17609125905568124, 'gold': 0.17609125905568124, 'arrived': 0.17609125905568124, 'truck': 0.17609125905568124}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il4fbXIzia_s",
        "colab_type": "text"
      },
      "source": [
        "**Übung: Berechnen Sie den TF-IDF Wert für einen anderen Korpus - also andere Sätze**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlOyIFSoimbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Ihr Code hier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoEwbJ0PpZ0d",
        "colab_type": "text"
      },
      "source": [
        "# Lektion 2: TF-IDF mit bereitgestelleter Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7NDqyefpe3W",
        "colab_type": "text"
      },
      "source": [
        "Anstatt TF-IDF selbst zu programmieren, können wir auch hier eine bereits bestehende Library für maschinelles Lernen namens ´sklearn´ verwenden. Wie Sie sehen wird hier automatisch tokeenisiert und lemmatisiert. Die Stoppwortliste üebrgeben wir bei der Initialisierung des vectorizer um Stoppworte zu entfernen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO0GA4uVp2wH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "0fa814bb-10bb-43d2-f874-e22649f8c76e"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=stoplist)\n",
        "corpus_new = [\n",
        "              'Shipment of gold damaged in a fire.',\n",
        "              'Delivery of silver arrived in a silver truck.',\n",
        "              'Shipment of gold arrived in a truck.']\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus_new)\n",
        "wort_index = vectorizer.vocabulary_\n",
        "idf = vectorizer.idf_\n",
        "print(wort_index)\n",
        "print(\"Dokument, Wortindex, TFID: \", tfidf_matrix)\n",
        "print(\"IDF wird hier mit einem anderen Logarithmus berechnet: \", dict(zip(vectorizer.get_feature_names(), idf)))\n",
        "print(\"TF-IDF daher auch etwas anders: \", dict(zip(vectorizer.get_feature_names(), np.asarray(tfidf_matrix.sum(axis=0)).ravel())))"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'shipment': 5, 'gold': 4, 'damaged': 1, 'fire': 3, 'delivery': 2, 'silver': 6, 'arrived': 0, 'truck': 7}\n",
            "Dokument, Wortindex, TFID:    (0, 3)\t0.5628290964997665\n",
            "  (0, 1)\t0.5628290964997665\n",
            "  (0, 4)\t0.42804603506311856\n",
            "  (0, 5)\t0.42804603506311856\n",
            "  (1, 7)\t0.3065042162415877\n",
            "  (1, 0)\t0.3065042162415877\n",
            "  (1, 6)\t0.8060324216071015\n",
            "  (1, 2)\t0.40301621080355077\n",
            "  (2, 7)\t0.5\n",
            "  (2, 0)\t0.5\n",
            "  (2, 4)\t0.5\n",
            "  (2, 5)\t0.5\n",
            "IDF wird hier mit einem anderen Logarithmus berechnet:  {'arrived': 1.2876820724517808, 'damaged': 1.6931471805599454, 'delivery': 1.6931471805599454, 'fire': 1.6931471805599454, 'gold': 1.2876820724517808, 'shipment': 1.2876820724517808, 'silver': 1.6931471805599454, 'truck': 1.2876820724517808}\n",
            "TF-IDF daher auch etwas anders:  {'arrived': 0.8065042162415876, 'damaged': 0.5628290964997665, 'delivery': 0.40301621080355077, 'fire': 0.5628290964997665, 'gold': 0.9280460350631186, 'shipment': 0.9280460350631186, 'silver': 0.8060324216071015, 'truck': 0.8065042162415876}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UZcFQZ1h7p4",
        "colab_type": "text"
      },
      "source": [
        "# Lektion 3: Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KjnxyB-u7tR",
        "colab_type": "text"
      },
      "source": [
        "Dieselbe Bibliothek, sklearn, kann auch für Clustering verwendet werden. Dazu verwenden wir die erstellten TF-IDF Vektoren. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-R5JojFvCnh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1a433525-f0ca-4b22-f8c0-5c85fa34f0a3"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def print_clusters(number_clusters, kmeans):\n",
        "  order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
        "  terms = vectorizer.get_feature_names()\n",
        "  for i in range(number_clusters):\n",
        "    print(\"Cluster %d:\" % i, end='')\n",
        "    for ind in order_centroids[i,:3]:\n",
        "      print(' %s' % terms[ind], end='')\n",
        "    print()\n",
        "\n",
        "\n",
        "# Wir müssen die Anzahl an erwarteten Clusterin eingeben - die optimale Zahl ist experimentell zu testen \n",
        "number_clusters = 2\n",
        "kmeans = KMeans(n_clusters=number_clusters, random_state=0).fit(tfidf_matrix)\n",
        "print_clusters(number_clusters, kmeans)"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster 0: shipment gold fire\n",
            "Cluster 1: silver delivery truck\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IaT3KYCrKjr",
        "colab_type": "text"
      },
      "source": [
        "**Übung: Passen Sie den Algorithmus oben auf Ihre eigenen Sätze an.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb_glsplrR23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Ihr Code hier"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}